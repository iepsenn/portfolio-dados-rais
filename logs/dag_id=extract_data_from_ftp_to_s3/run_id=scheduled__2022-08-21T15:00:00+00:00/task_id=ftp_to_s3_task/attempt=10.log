[2022-09-17 21:33:34,146] {taskinstance.py:1179} INFO - Dependencies all met for <TaskInstance: extract_data_from_ftp_to_s3.ftp_to_s3_task scheduled__2022-08-21T15:00:00+00:00 [queued]>
[2022-09-17 21:33:34,160] {taskinstance.py:1179} INFO - Dependencies all met for <TaskInstance: extract_data_from_ftp_to_s3.ftp_to_s3_task scheduled__2022-08-21T15:00:00+00:00 [queued]>
[2022-09-17 21:33:34,160] {taskinstance.py:1376} INFO - 
--------------------------------------------------------------------------------
[2022-09-17 21:33:34,160] {taskinstance.py:1377} INFO - Starting attempt 10 of 10
[2022-09-17 21:33:34,160] {taskinstance.py:1378} INFO - 
--------------------------------------------------------------------------------
[2022-09-17 21:33:34,178] {taskinstance.py:1397} INFO - Executing <Task(FTPToS3Operator): ftp_to_s3_task> on 2022-08-21 15:00:00+00:00
[2022-09-17 21:33:34,183] {standard_task_runner.py:52} INFO - Started process 9866 to run task
[2022-09-17 21:33:34,189] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'extract_data_from_ftp_to_s3', 'ftp_to_s3_task', 'scheduled__2022-08-21T15:00:00+00:00', '--job-id', '50', '--raw', '--subdir', 'DAGS_FOLDER/extract_data_from_ftp_to_s3.py', '--cfg-path', '/tmp/tmposb9mwd5', '--error-file', '/tmp/tmpql3f_4p8']
[2022-09-17 21:33:34,189] {standard_task_runner.py:80} INFO - Job 50: Subtask ftp_to_s3_task
[2022-09-17 21:33:34,266] {task_command.py:371} INFO - Running <TaskInstance: extract_data_from_ftp_to_s3.ftp_to_s3_task scheduled__2022-08-21T15:00:00+00:00 [running]> on host e1202d78ead4
[2022-09-17 21:33:34,373] {taskinstance.py:1591} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=extract_data_from_ftp_to_s3
AIRFLOW_CTX_TASK_ID=ftp_to_s3_task
AIRFLOW_CTX_EXECUTION_DATE=2022-08-21T15:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=10
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-08-21T15:00:00+00:00
[2022-09-17 21:33:34,386] {base.py:68} INFO - Using connection ID 'ftp_default' for task execution.
[2022-09-17 21:33:34,765] {ftp.py:184} INFO - Retrieving file from FTP: /pdet/microdados/CAGED/2007/CAGEDEST_122007.7z
[2022-09-17 21:34:45,586] {ftp.py:186} INFO - Finished retrieving file from FTP: /pdet/microdados/CAGED/2007/CAGEDEST_122007.7z
[2022-09-17 21:34:45,597] {base.py:68} INFO - Using connection ID 'aws_default' for task execution.
[2022-09-17 21:34:45,599] {base_aws.py:206} INFO - Credentials retrieved from login
[2022-09-17 21:34:51,809] {taskinstance.py:1909} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/urllib3/connection.py", line 175, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw
  File "/home/airflow/.local/lib/python3.7/site-packages/urllib3/util/connection.py", line 95, in create_connection
    raise err
  File "/home/airflow/.local/lib/python3.7/site-packages/urllib3/util/connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/httpsession.py", line 457, in send
    chunked=self._chunked(request.headers),
  File "/home/airflow/.local/lib/python3.7/site-packages/urllib3/connectionpool.py", line 786, in urlopen
    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]
  File "/home/airflow/.local/lib/python3.7/site-packages/urllib3/util/retry.py", line 525, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "/home/airflow/.local/lib/python3.7/site-packages/urllib3/packages/six.py", line 770, in reraise
    raise value
  File "/home/airflow/.local/lib/python3.7/site-packages/urllib3/connectionpool.py", line 710, in urlopen
    chunked=chunked,
  File "/home/airflow/.local/lib/python3.7/site-packages/urllib3/connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/urllib3/connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "/usr/local/lib/python3.7/http/client.py", line 1281, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/awsrequest.py", line 95, in _send_request
    method, url, body, headers, *args, **kwargs
  File "/usr/local/lib/python3.7/http/client.py", line 1327, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/usr/local/lib/python3.7/http/client.py", line 1276, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/awsrequest.py", line 123, in _send_output
    self.send(msg)
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/awsrequest.py", line 218, in send
    return super().send(str)
  File "/usr/local/lib/python3.7/http/client.py", line 976, in send
    self.connect()
  File "/home/airflow/.local/lib/python3.7/site-packages/urllib3/connection.py", line 205, in connect
    conn = self._new_conn()
  File "/home/airflow/.local/lib/python3.7/site-packages/urllib3/connection.py", line 187, in _new_conn
    self, "Failed to establish a new connection: %s" % e
urllib3.exceptions.NewConnectionError: <botocore.awsrequest.AWSHTTPConnection object at 0x7fc652ec7cd0>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/amazon/aws/transfers/ftp_to_s3.py", line 150, in execute
    self.__upload_to_s3_from_ftp(self.ftp_path, self.s3_key)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/amazon/aws/transfers/ftp_to_s3.py", line 109, in __upload_to_s3_from_ftp
    acl_policy=self.acl_policy,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/amazon/aws/hooks/s3.py", line 63, in wrapper
    return func(*bound_args.args, **bound_args.kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/amazon/aws/hooks/s3.py", line 91, in wrapper
    return func(*bound_args.args, **bound_args.kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/amazon/aws/hooks/s3.py", line 611, in load_file
    client.upload_file(filename, bucket_name, key, ExtraArgs=extra_args, Config=self.transfer_config)
  File "/home/airflow/.local/lib/python3.7/site-packages/boto3/s3/inject.py", line 148, in upload_file
    callback=Callback,
  File "/home/airflow/.local/lib/python3.7/site-packages/boto3/s3/transfer.py", line 288, in upload_file
    future.result()
  File "/home/airflow/.local/lib/python3.7/site-packages/s3transfer/futures.py", line 103, in result
    return self._coordinator.result()
  File "/home/airflow/.local/lib/python3.7/site-packages/s3transfer/futures.py", line 266, in result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/s3transfer/tasks.py", line 139, in __call__
    return self._execute_main(kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/s3transfer/tasks.py", line 162, in _execute_main
    return_value = self._main(**kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/s3transfer/tasks.py", line 349, in _main
    Bucket=bucket, Key=key, **extra_args
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/client.py", line 508, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/client.py", line 899, in _make_api_call
    operation_model, request_dict, request_context
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/client.py", line 921, in _make_request
    return self._endpoint.make_request(operation_model, request_dict)
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/endpoint.py", line 119, in make_request
    return self._send_request(request_dict, operation_model)
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/endpoint.py", line 207, in _send_request
    exception,
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/endpoint.py", line 361, in _needs_retry
    request_dict=request_dict,
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/hooks.py", line 412, in emit
    return self._emitter.emit(aliased_event_name, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/hooks.py", line 256, in emit
    return self._emit(event_name, kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/hooks.py", line 239, in _emit
    response = handler(**kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/retryhandler.py", line 207, in __call__
    if self._checker(**checker_kwargs):
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/retryhandler.py", line 285, in __call__
    attempt_number, response, caught_exception
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/retryhandler.py", line 320, in _should_retry
    return self._checker(attempt_number, response, caught_exception)
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/retryhandler.py", line 364, in __call__
    attempt_number, response, caught_exception
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/retryhandler.py", line 248, in __call__
    attempt_number, caught_exception
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/retryhandler.py", line 416, in _check_caught_exception
    raise caught_exception
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/endpoint.py", line 281, in _do_get_response
    http_response = self._send(request)
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/endpoint.py", line 377, in _send
    return self.http_session.send(request)
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/httpsession.py", line 477, in send
    raise EndpointConnectionError(endpoint_url=request.url, error=e)
botocore.exceptions.EndpointConnectionError: Could not connect to the endpoint URL: "http://localhost:9000/minio/caged/a.7z?uploads"
[2022-09-17 21:34:51,825] {taskinstance.py:1420} INFO - Marking task as FAILED. dag_id=extract_data_from_ftp_to_s3, task_id=ftp_to_s3_task, execution_date=20220821T150000, start_date=20220917T213334, end_date=20220917T213451
[2022-09-17 21:34:51,842] {standard_task_runner.py:97} ERROR - Failed to execute job 50 for task ftp_to_s3_task (Could not connect to the endpoint URL: "http://localhost:9000/minio/caged/a.7z?uploads"; 9866)
[2022-09-17 21:34:51,862] {local_task_job.py:156} INFO - Task exited with return code 1
[2022-09-17 21:34:51,905] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2022-09-17 21:34:51,933] {dagrun.py:549} ERROR - Marking run <DagRun extract_data_from_ftp_to_s3 @ 2022-08-21 15:00:00+00:00: scheduled__2022-08-21T15:00:00+00:00, externally triggered: False> failed
[2022-09-17 21:34:51,934] {dagrun.py:624} INFO - DagRun Finished: dag_id=extract_data_from_ftp_to_s3, execution_date=2022-08-21 15:00:00+00:00, run_id=scheduled__2022-08-21T15:00:00+00:00, run_start_date=2022-09-17 21:33:33.616281+00:00, run_end_date=2022-09-17 21:34:51.934369+00:00, run_duration=78.318088, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2022-08-21 15:00:00+00:00, data_interval_end=2022-08-21 16:00:00+00:00, dag_hash=2262387ffb5de1781fef637e8309a503
