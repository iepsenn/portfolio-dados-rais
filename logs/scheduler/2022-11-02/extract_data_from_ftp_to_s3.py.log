[2022-11-02 16:51:56,380] {processor.py:153} INFO - Started process (PID=46) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:51:56,381] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 16:51:56,383] {logging_mixin.py:115} INFO - [2022-11-02 16:51:56,382] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:51:57,054] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:51:57,091] {logging_mixin.py:115} INFO - [2022-11-02 16:51:57,091] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 16:51:57,137] {logging_mixin.py:115} INFO - [2022-11-02 16:51:57,137] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T15:00:00+00:00, run_after=2022-11-02T16:00:00+00:00
[2022-11-02 16:51:57,169] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.794 seconds
[2022-11-02 16:52:27,380] {processor.py:153} INFO - Started process (PID=140) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:52:27,385] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 16:52:27,386] {logging_mixin.py:115} INFO - [2022-11-02 16:52:27,386] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:52:28,023] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:52:28,104] {logging_mixin.py:115} INFO - [2022-11-02 16:52:28,104] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 16:52:28,190] {logging_mixin.py:115} INFO - [2022-11-02 16:52:28,190] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T15:00:00+00:00, run_after=2022-11-02T16:00:00+00:00
[2022-11-02 16:52:28,265] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.900 seconds
[2022-11-02 16:52:58,495] {processor.py:153} INFO - Started process (PID=196) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:52:58,496] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 16:52:58,497] {logging_mixin.py:115} INFO - [2022-11-02 16:52:58,497] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:52:58,888] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:52:58,923] {logging_mixin.py:115} INFO - [2022-11-02 16:52:58,923] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 16:52:58,987] {logging_mixin.py:115} INFO - [2022-11-02 16:52:58,987] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T15:00:00+00:00, run_after=2022-11-02T16:00:00+00:00
[2022-11-02 16:52:59,014] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.526 seconds
[2022-11-02 16:53:29,153] {processor.py:153} INFO - Started process (PID=249) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:53:29,155] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 16:53:29,156] {logging_mixin.py:115} INFO - [2022-11-02 16:53:29,156] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:53:29,566] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:53:29,611] {logging_mixin.py:115} INFO - [2022-11-02 16:53:29,611] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 16:53:29,668] {logging_mixin.py:115} INFO - [2022-11-02 16:53:29,667] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T15:00:00+00:00, run_after=2022-11-02T16:00:00+00:00
[2022-11-02 16:53:29,693] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.546 seconds
[2022-11-02 16:53:59,927] {processor.py:153} INFO - Started process (PID=303) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:53:59,929] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 16:53:59,931] {logging_mixin.py:115} INFO - [2022-11-02 16:53:59,931] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:54:00,405] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:54:00,439] {logging_mixin.py:115} INFO - [2022-11-02 16:54:00,439] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 16:54:00,482] {logging_mixin.py:115} INFO - [2022-11-02 16:54:00,482] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T15:00:00+00:00, run_after=2022-11-02T16:00:00+00:00
[2022-11-02 16:54:00,502] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.583 seconds
[2022-11-02 16:54:30,552] {processor.py:153} INFO - Started process (PID=356) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:54:30,560] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 16:54:30,561] {logging_mixin.py:115} INFO - [2022-11-02 16:54:30,561] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:54:31,142] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:54:31,191] {logging_mixin.py:115} INFO - [2022-11-02 16:54:31,190] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 16:54:31,258] {logging_mixin.py:115} INFO - [2022-11-02 16:54:31,257] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T15:00:00+00:00, run_after=2022-11-02T16:00:00+00:00
[2022-11-02 16:54:31,284] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.739 seconds
[2022-11-02 16:55:01,606] {processor.py:153} INFO - Started process (PID=419) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:55:01,608] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 16:55:01,609] {logging_mixin.py:115} INFO - [2022-11-02 16:55:01,609] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:55:02,132] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:55:02,166] {logging_mixin.py:115} INFO - [2022-11-02 16:55:02,166] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 16:55:02,214] {logging_mixin.py:115} INFO - [2022-11-02 16:55:02,213] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T15:00:00+00:00, run_after=2022-11-02T16:00:00+00:00
[2022-11-02 16:55:02,243] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.642 seconds
[2022-11-02 16:55:32,424] {processor.py:153} INFO - Started process (PID=473) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:55:32,426] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 16:55:32,427] {logging_mixin.py:115} INFO - [2022-11-02 16:55:32,427] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:55:32,892] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:55:32,929] {logging_mixin.py:115} INFO - [2022-11-02 16:55:32,929] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 16:55:32,991] {logging_mixin.py:115} INFO - [2022-11-02 16:55:32,990] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T15:00:00+00:00, run_after=2022-11-02T16:00:00+00:00
[2022-11-02 16:55:33,017] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.598 seconds
[2022-11-02 16:56:04,020] {processor.py:153} INFO - Started process (PID=528) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:56:04,021] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 16:56:04,023] {logging_mixin.py:115} INFO - [2022-11-02 16:56:04,023] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:56:04,547] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:56:04,584] {logging_mixin.py:115} INFO - [2022-11-02 16:56:04,584] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 16:56:04,638] {logging_mixin.py:115} INFO - [2022-11-02 16:56:04,637] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T15:00:00+00:00, run_after=2022-11-02T16:00:00+00:00
[2022-11-02 16:56:04,665] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.654 seconds
[2022-11-02 16:56:35,679] {processor.py:153} INFO - Started process (PID=591) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:56:35,680] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 16:56:35,681] {logging_mixin.py:115} INFO - [2022-11-02 16:56:35,681] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:56:36,090] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:56:36,124] {logging_mixin.py:115} INFO - [2022-11-02 16:56:36,123] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 16:56:36,178] {logging_mixin.py:115} INFO - [2022-11-02 16:56:36,178] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T15:00:00+00:00, run_after=2022-11-02T16:00:00+00:00
[2022-11-02 16:56:36,201] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.527 seconds
[2022-11-02 16:57:06,553] {processor.py:153} INFO - Started process (PID=645) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:57:06,555] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 16:57:06,556] {logging_mixin.py:115} INFO - [2022-11-02 16:57:06,556] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:57:07,062] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:57:07,098] {logging_mixin.py:115} INFO - [2022-11-02 16:57:07,097] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 16:57:07,147] {logging_mixin.py:115} INFO - [2022-11-02 16:57:07,146] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T15:00:00+00:00, run_after=2022-11-02T16:00:00+00:00
[2022-11-02 16:57:07,171] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.624 seconds
[2022-11-02 16:57:37,339] {processor.py:153} INFO - Started process (PID=699) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:57:37,341] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 16:57:37,342] {logging_mixin.py:115} INFO - [2022-11-02 16:57:37,342] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:57:37,812] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:57:37,850] {logging_mixin.py:115} INFO - [2022-11-02 16:57:37,850] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 16:57:37,901] {logging_mixin.py:115} INFO - [2022-11-02 16:57:37,901] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T15:00:00+00:00, run_after=2022-11-02T16:00:00+00:00
[2022-11-02 16:57:37,928] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.598 seconds
[2022-11-02 16:58:08,418] {processor.py:153} INFO - Started process (PID=762) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:58:08,419] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 16:58:08,420] {logging_mixin.py:115} INFO - [2022-11-02 16:58:08,420] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:58:08,838] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:58:08,882] {logging_mixin.py:115} INFO - [2022-11-02 16:58:08,881] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 16:58:08,939] {logging_mixin.py:115} INFO - [2022-11-02 16:58:08,939] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T15:00:00+00:00, run_after=2022-11-02T16:00:00+00:00
[2022-11-02 16:58:08,963] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.552 seconds
[2022-11-02 16:58:39,572] {processor.py:153} INFO - Started process (PID=816) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:58:39,573] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 16:58:39,574] {logging_mixin.py:115} INFO - [2022-11-02 16:58:39,574] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:58:40,055] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:58:40,087] {logging_mixin.py:115} INFO - [2022-11-02 16:58:40,087] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 16:58:40,134] {logging_mixin.py:115} INFO - [2022-11-02 16:58:40,133] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T15:00:00+00:00, run_after=2022-11-02T16:00:00+00:00
[2022-11-02 16:58:40,159] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.593 seconds
[2022-11-02 16:59:10,591] {processor.py:153} INFO - Started process (PID=876) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:59:10,593] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 16:59:10,595] {logging_mixin.py:115} INFO - [2022-11-02 16:59:10,594] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:59:11,194] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:59:11,239] {logging_mixin.py:115} INFO - [2022-11-02 16:59:11,239] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 16:59:11,301] {logging_mixin.py:115} INFO - [2022-11-02 16:59:11,301] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T15:00:00+00:00, run_after=2022-11-02T16:00:00+00:00
[2022-11-02 16:59:11,341] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.757 seconds
[2022-11-02 16:59:41,608] {processor.py:153} INFO - Started process (PID=932) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:59:41,612] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 16:59:41,613] {logging_mixin.py:115} INFO - [2022-11-02 16:59:41,613] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:59:42,001] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 16:59:42,035] {logging_mixin.py:115} INFO - [2022-11-02 16:59:42,034] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 16:59:42,076] {logging_mixin.py:115} INFO - [2022-11-02 16:59:42,076] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T15:00:00+00:00, run_after=2022-11-02T16:00:00+00:00
[2022-11-02 16:59:42,094] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.494 seconds
[2022-11-02 17:00:12,653] {processor.py:153} INFO - Started process (PID=984) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:00:12,654] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 17:00:12,656] {logging_mixin.py:115} INFO - [2022-11-02 17:00:12,656] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:00:13,118] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:00:13,155] {logging_mixin.py:115} INFO - [2022-11-02 17:00:13,154] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 17:00:13,202] {logging_mixin.py:115} INFO - [2022-11-02 17:00:13,202] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T16:00:00+00:00, run_after=2022-11-02T17:00:00+00:00
[2022-11-02 17:00:13,226] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.580 seconds
[2022-11-02 17:00:43,425] {processor.py:153} INFO - Started process (PID=1038) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:00:43,429] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 17:00:43,430] {logging_mixin.py:115} INFO - [2022-11-02 17:00:43,430] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:00:43,893] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:00:43,934] {logging_mixin.py:115} INFO - [2022-11-02 17:00:43,934] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 17:00:43,981] {logging_mixin.py:115} INFO - [2022-11-02 17:00:43,981] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T16:00:00+00:00, run_after=2022-11-02T17:00:00+00:00
[2022-11-02 17:00:44,004] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.585 seconds
[2022-11-02 17:01:14,312] {processor.py:153} INFO - Started process (PID=1100) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:01:14,314] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 17:01:14,316] {logging_mixin.py:115} INFO - [2022-11-02 17:01:14,315] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:01:14,973] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:01:15,038] {logging_mixin.py:115} INFO - [2022-11-02 17:01:15,038] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 17:01:15,123] {logging_mixin.py:115} INFO - [2022-11-02 17:01:15,123] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T16:00:00+00:00, run_after=2022-11-02T17:00:00+00:00
[2022-11-02 17:01:15,164] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.860 seconds
[2022-11-02 17:01:45,408] {processor.py:153} INFO - Started process (PID=1154) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:01:45,409] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 17:01:45,410] {logging_mixin.py:115} INFO - [2022-11-02 17:01:45,410] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:01:45,817] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:01:45,850] {logging_mixin.py:115} INFO - [2022-11-02 17:01:45,850] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 17:01:45,891] {logging_mixin.py:115} INFO - [2022-11-02 17:01:45,891] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T16:00:00+00:00, run_after=2022-11-02T17:00:00+00:00
[2022-11-02 17:01:45,909] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.508 seconds
[2022-11-02 17:02:56,545] {processor.py:153} INFO - Started process (PID=1201) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:02:56,548] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 17:02:56,551] {logging_mixin.py:115} INFO - [2022-11-02 17:02:56,550] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:02:57,371] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:02:57,436] {logging_mixin.py:115} INFO - [2022-11-02 17:02:57,436] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 17:02:57,514] {logging_mixin.py:115} INFO - [2022-11-02 17:02:57,514] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T16:00:00+00:00, run_after=2022-11-02T17:00:00+00:00
[2022-11-02 17:02:57,563] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 1.027 seconds
[2022-11-02 17:03:28,190] {processor.py:153} INFO - Started process (PID=1256) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:03:28,194] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 17:03:28,195] {logging_mixin.py:115} INFO - [2022-11-02 17:03:28,195] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:03:28,577] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:03:28,615] {logging_mixin.py:115} INFO - [2022-11-02 17:03:28,615] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 17:03:28,657] {logging_mixin.py:115} INFO - [2022-11-02 17:03:28,657] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T16:00:00+00:00, run_after=2022-11-02T17:00:00+00:00
[2022-11-02 17:03:28,678] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.493 seconds
[2022-11-02 17:03:59,166] {processor.py:153} INFO - Started process (PID=1310) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:03:59,167] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 17:03:59,168] {logging_mixin.py:115} INFO - [2022-11-02 17:03:59,168] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:03:59,625] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:03:59,661] {logging_mixin.py:115} INFO - [2022-11-02 17:03:59,661] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 17:03:59,708] {logging_mixin.py:115} INFO - [2022-11-02 17:03:59,708] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T16:00:00+00:00, run_after=2022-11-02T17:00:00+00:00
[2022-11-02 17:03:59,731] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.574 seconds
[2022-11-02 17:04:29,878] {processor.py:153} INFO - Started process (PID=1372) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:04:29,883] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 17:04:29,885] {logging_mixin.py:115} INFO - [2022-11-02 17:04:29,885] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:04:30,631] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:04:30,667] {logging_mixin.py:115} INFO - [2022-11-02 17:04:30,667] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 17:04:30,719] {logging_mixin.py:115} INFO - [2022-11-02 17:04:30,718] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T16:00:00+00:00, run_after=2022-11-02T17:00:00+00:00
[2022-11-02 17:04:30,742] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.873 seconds
[2022-11-02 17:05:01,197] {processor.py:153} INFO - Started process (PID=1427) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:05:01,198] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 17:05:01,199] {logging_mixin.py:115} INFO - [2022-11-02 17:05:01,199] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:05:01,707] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:05:01,747] {logging_mixin.py:115} INFO - [2022-11-02 17:05:01,747] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 17:05:01,803] {logging_mixin.py:115} INFO - [2022-11-02 17:05:01,803] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T16:00:00+00:00, run_after=2022-11-02T17:00:00+00:00
[2022-11-02 17:05:01,836] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.644 seconds
[2022-11-02 17:05:32,114] {processor.py:153} INFO - Started process (PID=1480) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:05:32,115] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 17:05:32,115] {logging_mixin.py:115} INFO - [2022-11-02 17:05:32,115] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:05:32,559] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:05:32,593] {logging_mixin.py:115} INFO - [2022-11-02 17:05:32,593] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 17:05:32,639] {logging_mixin.py:115} INFO - [2022-11-02 17:05:32,639] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T16:00:00+00:00, run_after=2022-11-02T17:00:00+00:00
[2022-11-02 17:05:32,666] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.556 seconds
[2022-11-02 17:06:03,117] {processor.py:153} INFO - Started process (PID=1540) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:06:03,119] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 17:06:03,120] {logging_mixin.py:115} INFO - [2022-11-02 17:06:03,120] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:06:03,919] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:06:03,995] {logging_mixin.py:115} INFO - [2022-11-02 17:06:03,995] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 17:06:04,092] {logging_mixin.py:115} INFO - [2022-11-02 17:06:04,092] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T16:00:00+00:00, run_after=2022-11-02T17:00:00+00:00
[2022-11-02 17:06:04,135] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 1.028 seconds
[2022-11-02 17:06:34,414] {processor.py:153} INFO - Started process (PID=1595) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:06:34,416] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 17:06:34,417] {logging_mixin.py:115} INFO - [2022-11-02 17:06:34,417] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:06:34,988] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:06:35,020] {logging_mixin.py:115} INFO - [2022-11-02 17:06:35,019] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 17:06:35,068] {logging_mixin.py:115} INFO - [2022-11-02 17:06:35,067] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T16:00:00+00:00, run_after=2022-11-02T17:00:00+00:00
[2022-11-02 17:06:35,085] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.677 seconds
[2022-11-02 17:07:05,464] {processor.py:153} INFO - Started process (PID=1650) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:07:05,467] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 17:07:05,468] {logging_mixin.py:115} INFO - [2022-11-02 17:07:05,468] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:07:06,090] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:07:06,140] {logging_mixin.py:115} INFO - [2022-11-02 17:07:06,139] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 17:07:06,202] {logging_mixin.py:115} INFO - [2022-11-02 17:07:06,201] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T16:00:00+00:00, run_after=2022-11-02T17:00:00+00:00
[2022-11-02 17:07:06,236] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.779 seconds
[2022-11-02 17:07:36,597] {processor.py:153} INFO - Started process (PID=1704) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:07:36,598] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 17:07:36,599] {logging_mixin.py:115} INFO - [2022-11-02 17:07:36,599] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:07:37,096] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:07:37,139] {logging_mixin.py:115} INFO - [2022-11-02 17:07:37,139] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 17:07:37,191] {logging_mixin.py:115} INFO - [2022-11-02 17:07:37,190] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T16:00:00+00:00, run_after=2022-11-02T17:00:00+00:00
[2022-11-02 17:07:37,216] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.626 seconds
[2022-11-02 17:08:07,265] {processor.py:153} INFO - Started process (PID=1767) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:08:07,266] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 17:08:07,267] {logging_mixin.py:115} INFO - [2022-11-02 17:08:07,267] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:08:07,620] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:08:07,649] {logging_mixin.py:115} INFO - [2022-11-02 17:08:07,649] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 17:08:07,703] {logging_mixin.py:115} INFO - [2022-11-02 17:08:07,703] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T16:00:00+00:00, run_after=2022-11-02T17:00:00+00:00
[2022-11-02 17:08:07,740] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.479 seconds
[2022-11-02 17:08:38,018] {processor.py:153} INFO - Started process (PID=1819) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:08:38,019] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 17:08:38,021] {logging_mixin.py:115} INFO - [2022-11-02 17:08:38,021] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:08:38,516] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:08:38,554] {logging_mixin.py:115} INFO - [2022-11-02 17:08:38,554] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 17:08:38,601] {logging_mixin.py:115} INFO - [2022-11-02 17:08:38,601] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T16:00:00+00:00, run_after=2022-11-02T17:00:00+00:00
[2022-11-02 17:08:38,628] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.616 seconds
[2022-11-02 17:09:08,876] {processor.py:153} INFO - Started process (PID=1873) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:09:08,880] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 17:09:08,882] {logging_mixin.py:115} INFO - [2022-11-02 17:09:08,881] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:09:09,300] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:09:09,333] {logging_mixin.py:115} INFO - [2022-11-02 17:09:09,333] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 17:09:09,379] {logging_mixin.py:115} INFO - [2022-11-02 17:09:09,379] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T16:00:00+00:00, run_after=2022-11-02T17:00:00+00:00
[2022-11-02 17:09:09,405] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.537 seconds
[2022-11-02 17:09:39,761] {processor.py:153} INFO - Started process (PID=1926) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:09:39,765] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 17:09:39,768] {logging_mixin.py:115} INFO - [2022-11-02 17:09:39,768] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:09:40,344] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:09:40,383] {logging_mixin.py:115} INFO - [2022-11-02 17:09:40,383] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 17:09:40,452] {logging_mixin.py:115} INFO - [2022-11-02 17:09:40,452] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T16:00:00+00:00, run_after=2022-11-02T17:00:00+00:00
[2022-11-02 17:09:40,479] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.725 seconds
[2022-11-02 17:10:11,014] {processor.py:153} INFO - Started process (PID=1988) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:10:11,018] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 17:10:11,020] {logging_mixin.py:115} INFO - [2022-11-02 17:10:11,020] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:10:11,577] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:10:11,642] {logging_mixin.py:115} INFO - [2022-11-02 17:10:11,642] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 17:10:11,787] {logging_mixin.py:115} INFO - [2022-11-02 17:10:11,787] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T16:00:00+00:00, run_after=2022-11-02T17:00:00+00:00
[2022-11-02 17:10:11,837] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.830 seconds
[2022-11-02 17:10:41,894] {processor.py:153} INFO - Started process (PID=2042) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:10:41,899] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 17:10:41,900] {logging_mixin.py:115} INFO - [2022-11-02 17:10:41,899] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:10:42,435] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:10:42,478] {logging_mixin.py:115} INFO - [2022-11-02 17:10:42,478] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 17:10:42,531] {logging_mixin.py:115} INFO - [2022-11-02 17:10:42,531] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T16:00:00+00:00, run_after=2022-11-02T17:00:00+00:00
[2022-11-02 17:10:42,558] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.673 seconds
[2022-11-02 17:11:12,966] {processor.py:153} INFO - Started process (PID=2096) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:11:12,967] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 17:11:12,968] {logging_mixin.py:115} INFO - [2022-11-02 17:11:12,968] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:11:13,505] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:11:13,558] {logging_mixin.py:115} INFO - [2022-11-02 17:11:13,558] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 17:11:13,643] {logging_mixin.py:115} INFO - [2022-11-02 17:11:13,642] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T16:00:00+00:00, run_after=2022-11-02T17:00:00+00:00
[2022-11-02 17:11:13,680] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.724 seconds
[2022-11-02 17:11:44,197] {processor.py:153} INFO - Started process (PID=2157) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:11:44,198] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 17:11:44,199] {logging_mixin.py:115} INFO - [2022-11-02 17:11:44,199] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:11:44,828] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:11:44,881] {logging_mixin.py:115} INFO - [2022-11-02 17:11:44,880] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 17:11:44,951] {logging_mixin.py:115} INFO - [2022-11-02 17:11:44,950] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T16:00:00+00:00, run_after=2022-11-02T17:00:00+00:00
[2022-11-02 17:11:44,980] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.788 seconds
[2022-11-02 17:12:15,478] {processor.py:153} INFO - Started process (PID=2213) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:12:15,479] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 17:12:15,480] {logging_mixin.py:115} INFO - [2022-11-02 17:12:15,480] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:12:15,904] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:12:15,941] {logging_mixin.py:115} INFO - [2022-11-02 17:12:15,941] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 17:12:15,996] {logging_mixin.py:115} INFO - [2022-11-02 17:12:15,996] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T16:00:00+00:00, run_after=2022-11-02T17:00:00+00:00
[2022-11-02 17:12:16,021] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.551 seconds
[2022-11-02 17:12:46,296] {processor.py:153} INFO - Started process (PID=2267) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:12:46,300] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 17:12:46,302] {logging_mixin.py:115} INFO - [2022-11-02 17:12:46,301] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:12:47,008] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:12:47,063] {logging_mixin.py:115} INFO - [2022-11-02 17:12:47,063] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 17:12:47,127] {logging_mixin.py:115} INFO - [2022-11-02 17:12:47,126] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T16:00:00+00:00, run_after=2022-11-02T17:00:00+00:00
[2022-11-02 17:12:47,154] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.865 seconds
[2022-11-02 17:13:17,377] {processor.py:153} INFO - Started process (PID=2320) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:13:17,382] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 17:13:17,385] {logging_mixin.py:115} INFO - [2022-11-02 17:13:17,384] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:13:17,944] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:13:17,990] {logging_mixin.py:115} INFO - [2022-11-02 17:13:17,989] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 17:13:18,053] {logging_mixin.py:115} INFO - [2022-11-02 17:13:18,053] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T16:00:00+00:00, run_after=2022-11-02T17:00:00+00:00
[2022-11-02 17:13:18,093] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.720 seconds
[2022-11-02 17:13:48,254] {processor.py:153} INFO - Started process (PID=2379) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:13:48,256] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 17:13:48,257] {logging_mixin.py:115} INFO - [2022-11-02 17:13:48,257] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:13:49,056] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:13:49,103] {logging_mixin.py:115} INFO - [2022-11-02 17:13:49,103] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 17:13:49,172] {logging_mixin.py:115} INFO - [2022-11-02 17:13:49,172] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T16:00:00+00:00, run_after=2022-11-02T17:00:00+00:00
[2022-11-02 17:13:49,207] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.960 seconds
[2022-11-02 17:14:19,298] {processor.py:153} INFO - Started process (PID=2436) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:14:19,303] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 17:14:19,305] {logging_mixin.py:115} INFO - [2022-11-02 17:14:19,304] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:14:19,875] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:14:19,924] {logging_mixin.py:115} INFO - [2022-11-02 17:14:19,924] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 17:14:19,993] {logging_mixin.py:115} INFO - [2022-11-02 17:14:19,993] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T16:00:00+00:00, run_after=2022-11-02T17:00:00+00:00
[2022-11-02 17:14:20,029] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.741 seconds
[2022-11-02 17:14:50,749] {processor.py:153} INFO - Started process (PID=2492) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:14:50,751] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 17:14:50,754] {logging_mixin.py:115} INFO - [2022-11-02 17:14:50,753] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:14:51,515] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:14:51,616] {logging_mixin.py:115} INFO - [2022-11-02 17:14:51,614] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 17:14:51,693] {logging_mixin.py:115} INFO - [2022-11-02 17:14:51,693] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T16:00:00+00:00, run_after=2022-11-02T17:00:00+00:00
[2022-11-02 17:14:51,741] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 1.001 seconds
[2022-11-02 17:15:22,105] {processor.py:153} INFO - Started process (PID=2547) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:15:22,107] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 17:15:22,108] {logging_mixin.py:115} INFO - [2022-11-02 17:15:22,108] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:15:22,613] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:15:22,648] {logging_mixin.py:115} INFO - [2022-11-02 17:15:22,648] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 17:15:22,697] {logging_mixin.py:115} INFO - [2022-11-02 17:15:22,697] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T16:00:00+00:00, run_after=2022-11-02T17:00:00+00:00
[2022-11-02 17:15:22,720] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.626 seconds
[2022-11-02 17:15:52,846] {processor.py:153} INFO - Started process (PID=2609) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:15:52,847] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 17:15:52,848] {logging_mixin.py:115} INFO - [2022-11-02 17:15:52,848] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:15:53,343] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:15:53,374] {logging_mixin.py:115} INFO - [2022-11-02 17:15:53,374] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 17:15:53,416] {logging_mixin.py:115} INFO - [2022-11-02 17:15:53,416] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T16:00:00+00:00, run_after=2022-11-02T17:00:00+00:00
[2022-11-02 17:15:53,437] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.596 seconds
[2022-11-02 17:16:23,557] {processor.py:153} INFO - Started process (PID=2665) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:16:23,558] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 17:16:23,559] {logging_mixin.py:115} INFO - [2022-11-02 17:16:23,558] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:16:23,976] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:16:24,011] {logging_mixin.py:115} INFO - [2022-11-02 17:16:24,011] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 17:16:24,053] {logging_mixin.py:115} INFO - [2022-11-02 17:16:24,053] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T16:00:00+00:00, run_after=2022-11-02T17:00:00+00:00
[2022-11-02 17:16:24,074] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.522 seconds
[2022-11-02 17:16:54,212] {processor.py:153} INFO - Started process (PID=2717) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:16:54,214] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 17:16:54,214] {logging_mixin.py:115} INFO - [2022-11-02 17:16:54,214] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:16:54,586] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:16:54,620] {logging_mixin.py:115} INFO - [2022-11-02 17:16:54,620] {dag.py:2420} INFO - Sync 1 DAGs
[2022-11-02 17:16:54,659] {logging_mixin.py:115} INFO - [2022-11-02 17:16:54,659] {dag.py:2972} INFO - Setting next_dagrun for extract_data_from_ftp_to_s3 to 2022-11-02T16:00:00+00:00, run_after=2022-11-02T17:00:00+00:00
[2022-11-02 17:16:54,683] {processor.py:161} INFO - Processing /opt/airflow/dags/extract_data_from_ftp_to_s3.py took 0.475 seconds
[2022-11-02 17:17:25,772] {processor.py:153} INFO - Started process (PID=2770) to work on /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:17:25,774] {processor.py:641} INFO - Processing file /opt/airflow/dags/extract_data_from_ftp_to_s3.py for tasks to queue
[2022-11-02 17:17:25,777] {logging_mixin.py:115} INFO - [2022-11-02 17:17:25,776] {dagbag.py:508} INFO - Filling up the DagBag from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:17:28,495] {logging_mixin.py:115} INFO - [2022-11-02 17:17:28,495] {process_utils.py:240} INFO - Waiting up to 5 seconds for processes to exit...
[2022-11-02 17:17:28,496] {logging_mixin.py:115} WARNING - Exception ignored in: <function WeakKeyDictionary.__init__.<locals>.remove at 0x7f8050a0cdd0>
[2022-11-02 17:17:28,496] {logging_mixin.py:115} WARNING - Traceback (most recent call last):
[2022-11-02 17:17:28,496] {logging_mixin.py:115} WARNING -   File "/usr/local/lib/python3.7/weakref.py", line 358, in remove
[2022-11-02 17:17:28,496] {logging_mixin.py:115} WARNING -     def remove(k, selfref=ref(self)):
[2022-11-02 17:17:28,496] {logging_mixin.py:115} WARNING -   File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/manager.py", line 463, in _exit_gracefully
[2022-11-02 17:17:28,496] {logging_mixin.py:115} WARNING -     self.log.debug("Current Stacktrace is: %s", '\n'.join(map(str, inspect.stack())))
[2022-11-02 17:17:28,497] {logging_mixin.py:115} WARNING -   File "/usr/local/lib/python3.7/inspect.py", line 1513, in stack
[2022-11-02 17:17:28,497] {logging_mixin.py:115} WARNING -     return getouterframes(sys._getframe(1), context)
[2022-11-02 17:17:28,497] {logging_mixin.py:115} WARNING -   File "/usr/local/lib/python3.7/inspect.py", line 1490, in getouterframes
[2022-11-02 17:17:28,498] {logging_mixin.py:115} WARNING -     frameinfo = (frame,) + getframeinfo(frame, context)
[2022-11-02 17:17:28,498] {logging_mixin.py:115} WARNING -   File "/usr/local/lib/python3.7/inspect.py", line 1460, in getframeinfo
[2022-11-02 17:17:28,499] {logging_mixin.py:115} WARNING -     filename = getsourcefile(frame) or getfile(frame)
[2022-11-02 17:17:28,499] {logging_mixin.py:115} WARNING -   File "/usr/local/lib/python3.7/inspect.py", line 693, in getsourcefile
[2022-11-02 17:17:28,499] {logging_mixin.py:115} WARNING -     if os.path.exists(filename):
[2022-11-02 17:17:28,499] {logging_mixin.py:115} WARNING -   File "/usr/local/lib/python3.7/genericpath.py", line 19, in exists
[2022-11-02 17:17:28,499] {logging_mixin.py:115} WARNING -     os.stat(path)
[2022-11-02 17:17:28,499] {logging_mixin.py:115} WARNING -   File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/manager.py", line 467, in _exit_gracefully
[2022-11-02 17:17:28,500] {logging_mixin.py:115} WARNING -     sys.exit(os.EX_OK)
[2022-11-02 17:17:28,500] {logging_mixin.py:115} WARNING - SystemExit: 0
[2022-11-02 17:17:28,662] {processor.py:651} INFO - DAG(s) dict_keys(['extract_data_from_ftp_to_s3']) retrieved from /opt/airflow/dags/extract_data_from_ftp_to_s3.py
[2022-11-02 17:17:29,389] {processor.py:164} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 72, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 72, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.7/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _run_file_processor
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 71, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 660, in process_file
    dagbag.sync_to_db()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 71, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 616, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 382, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 360, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 193, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 628, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 602, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 68, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 142, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2897, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1530, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 3247, in _wrap_pool_connect
    e, dialect, self
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 2101, in _handle_dbapi_exception_noconnection
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 72, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 72, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.7/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
